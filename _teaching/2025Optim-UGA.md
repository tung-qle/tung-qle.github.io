---
title: "Numerical Optimisation"
collection: teaching
type: "Master 1"
venue: "Université Grenoble Alpes, UFR IM2AG"
start_date: January 2026
end_date: April 2026
year: 2025-2026
location: "Grenoble, France"
position: Lecturer
---

## Overview
<div style='text-align: justify;'>
This is the webpage of the course <b>Numerical Optimization</b>, Master 1 of Applied Mathematics, at the Université Grenoble Alpes, France.
<br>
The course aims to equipe students with basic knowledge of numerical optimization: mathematical background, important concepts, main algorithms, their implementation and theoretical guarantees.
</div>

## Course organisation
<div style='text-align: justify;'>
This course will take place during 11 weeks. Each week, there are <b>one lecture</b>, one tutorial (<b>TD</b>) and one practical session (<b>TP</b>), each of which lasts 1 hour 30 minutes. 
<br>
Unless specified otherwise, place and time slot for these sessions are:
</div>

<ul style='text-align: justify;'>
<li> <b>TP</b>: 9h45AM Monday, room F202-IM2AG and will be given by <a href="https://tguilmeau.github.io/">Thomas Guillaume</a></li>
<li> <b>Lecture and TD</b>: 9h45AM Thursday, room F321-IM2AG and will be given by myself</li> 
</ul>

## Course evaluation
<div style='text-align: justify;'>
The final evaluation will be based on pratical sessions and a final exam. <b>Only a couple</b> of practical sessions will be noted (and you will be informed of those sessions). 
<br>
<b>Note for students from graduate school</b>: your final evaluation will be based additionally on another <b>mini-project</b>.  
</div>

## Course materials
<div style='text-align: justify;'>
It is worth noticing that lectures and TDs are designed to greatly complement each other. While TPs are somewhat designed independently, they also follow closely the lectures.
<br>
Materials for TPs can be found at: <b>to be updated</b>.
<!-- <a href="https://github.com/wazizian/NumericalOptimization">https://github.com/wazizian/NumericalOptimization</a>.   -->
<br>
Materials for lectures and TDs can be found below:
</div>

<ul style='text-align: justify;'>
<li> Lecture 1: <i>Introduction and refresher course</i> </li>
<li> Lecture 2: <i>Convex functions and sets</i> </li> 
<li> Lecture 3: <i>Gradient descent and theoretical properties</i> </li> 
<li> Lecture 4: <i>Lower-bound of first-order methods and Nesterov optimal algorithm</i> </li> 
<li> Lecture 5: <i>Stochastic gradient descent and theoretical properties</i> </li> 
<li> Lecture 6: <i>Adaptative methods - Adagrad and variants</i> </li> 
<li> Lecture 7: <i>Second-order methods - Newton algorithm</i> </li> 
<li> Lecture 8:  </li> 
<li> Lecture 9:  </li> 
<li> Lecture 10:  </li> 
<li> Lecture 11:  </li> 
</ul>

## Course references
While I do not use a specific textbook to prepare for the lectures, you are welcome to read these following references to follow the course more easily.

<ul style='text-align: justify;'>
<li> <b>Numerical optimization</b> - <i>Jorge Nocedal</i> & <i>Stephen J. Wright</i> </li>
<li> <b>Convex optimization</b> - <i>Stephen Boyd</i> & <i>Lieven Vandenberghe</i> </li> 
<li> <b>Introduction to Optimization</b> - <i>Boris Teodorovitsj Polyak</i> </li> 
<li> <b>Introductory Lectures on Convex Optimization: a basic course</b> - <i>Yurii Nesterov</i> </li> 
</ul>
