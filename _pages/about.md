---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

 I am a *Maître de Conférences (MCF)* (equivalent to Assistant/Associate Professor in the American system) at Université Grenoble Alpes (UGA). I am a member of team [DAO](https://dao-ljk.imag.fr/). Our research team focus on methodological and theoretical aspects of optimization and machine learning. 
{: .text-justify}

Contact: quoc-tung [DOT] le [at] univ-grenoble-alpes [DOT] fr

# Research interests

I am broadly interested in __*deep learning theory*__, __*non-convex*__/__*non-smooth optimization*__, __*learning with sparsity and structures*__. I am also into other subjects in __*Theoretical Computer Science*__.
{: .text-justify}

# Curriculum
*September 2025 -* : Assistant Professor (MCF) at Université Grenoble Alpes (UGA), France. 
{: .text-justify}

*January 2024 - August 2025*: Postdoc at TSE (Toulouse School of Economics, France), working with [Jérôme Bolte](http://bolte.perso.math.cnrs.fr/) and [Edouard Pauwels](https://www.irit.fr/~Edouard.Pauwels/) on semi-algebraic optimization and its application to bilevel optimization and Machine Learning.  
{: .text-justify}

*September 2020 - December 2023*: PhD student in [OCKHAM team](https://team.inria.fr/dante/fr/), ENS Lyon, France. I am under the supervision of [Elisa Riccietti](http://perso.ens-lyon.fr/elisa.riccietti/) and [Rémi Gribonval](https://people.irisa.fr/Remi.Gribonval/). 
{: .text-justify}

*September 2017 - August 2020*: Bachelor and Masters at Ecole Normale Supérieure de Paris (ENS de Paris), France. I have a major in Computer Science and a minor in Mathematics
{: .text-justify}

*September 2014 - August 2017*: Engineering program at Hanoi University of Science and Technology (HUST), Vietnam.
{: .text-justify}
<!-- # My PhD project

My PhD project considers several algorithmic and theoretical aspects of the training problem of sparse deep neural networks (DNNs) and the problem of sparse matrix factorization. In comparison to usual DNNs, Sparse DNNs have *sparse* weight matrices. Analoguously, in comparison to matrix factorization, sparse matrix factorization approximate a target matrix by product of (multiple) *sparse* factors.
{: .text-justify}

For sparse DNNs, we study the existence of optimal solutions of their training problems. In addition, our study investigates several topological properties of the function space associated to sparse DNN architectures such as (non-)closedness. 
{: .text-justify}

For Sparse Matrix Factorization, we study in detail one of its variant named "Fixed support matrix factorization" in which the knowledge of the supports (the set of non-zero entries) of factors are avaiblable and the factorization consists of optimizing the coefficients in the supports to minimize the distance between the target matrix and the product of sparse factors. We obtained results on the complexity, algorithms and landscape of the loss function for the case of two factors. Those results are extended to the case of multiple factors under the assumption that the supports of the factor admit the "butterfly structure" (see illustration below). This allows us to not only analyze several sparsity structures proposed for deep learning models but also establish a new theoretical guarantee for the so-called "butterfly algorithm".
{: .text-justify}

<figure>
  <img
    src="/images/hadamard.png"
     alt="An example of sparse matrix factorization"
     class="img-responsive"
     style="float: center; 
      margin-top: 1em;"
    >
    <figcaption>Butterfly structure of the Hadamard transformation</figcaption>
</figure> -->

